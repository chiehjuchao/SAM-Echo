{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# MedSAM Inference\n",
    "\n",
    "This notebook provides a clean implementation for MedSAM inference on medical images.\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- segment-anything\n",
    "- numpy\n",
    "- matplotlib\n",
    "- opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Environment and functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "# Source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    \"\"\"Display segmentation mask on matplotlib axis.\n",
    "    \n",
    "    Args:\n",
    "        mask: Binary mask array (H, W)\n",
    "        ax: Matplotlib axis object\n",
    "        random_color: If True, use random color; otherwise use yellow\n",
    "    \"\"\"\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    \"\"\"Display bounding box on matplotlib axis.\n",
    "    \n",
    "    Args:\n",
    "        box: Bounding box coordinates [x0, y0, x1, y1]\n",
    "        ax: Matplotlib axis object\n",
    "    \"\"\"\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## MedSAM Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    \"\"\"Perform MedSAM inference given image embeddings and bounding box.\n",
    "    \n",
    "    Args:\n",
    "        medsam_model: MedSAM model instance\n",
    "        img_embed: Image embeddings from encoder (B, 256, 64, 64)\n",
    "        box_1024: Bounding box in 1024x1024 scale (B, 4)\n",
    "        H: Target height for output mask\n",
    "        W: Target width for output mask\n",
    "        \n",
    "    Returns:\n",
    "        Binary segmentation mask (H, W)\n",
    "    \"\"\"\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    \n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed,  # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, H, W)\n",
    "    \n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (H, W)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-header",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_from_mask(mask):\n",
    "    \"\"\"Extract the largest bounding box from a binary mask.\n",
    "    \n",
    "    Args:\n",
    "        mask: Binary mask (can be numpy array or torch tensor)\n",
    "        \n",
    "    Returns:\n",
    "        Bounding box coordinates [x0, y0, x1, y1] or None if no contours found\n",
    "    \"\"\"\n",
    "    # Convert the mask to a numpy array of type uint8\n",
    "    if torch.is_tensor(mask):\n",
    "        mask_np = (mask.cpu().numpy() == 1).astype(np.uint8)\n",
    "    else:\n",
    "        mask_np = (mask == 1).astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize variables to keep track of the largest bounding box and its area\n",
    "    largest_area = 0\n",
    "    largest_bbox = None\n",
    "\n",
    "    for contour in contours:\n",
    "        # Calculate the area of the current bounding box\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = w * h\n",
    "\n",
    "        # If the current bounding box has a larger area, update\n",
    "        if area > largest_area:\n",
    "            largest_area = area\n",
    "            largest_bbox = [x, y, x + w, y + h]\n",
    "\n",
    "    if largest_bbox is not None:\n",
    "        # Convert the coordinates to floating-point numbers\n",
    "        largest_bbox = [float(coord) for coord in largest_bbox]\n",
    "\n",
    "    return largest_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-loading-header",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Choose one of the following methods to load your MedSAM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-loading-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load model with checkpoint directly (for older checkpoints)\n",
    "MedSAM_CKPT_PATH = \"path/to/medsam_model_best.pth\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-loading-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load model with separate pretrained weights and trained checkpoint\n",
    "# (Uncomment if you need this method)\n",
    "\n",
    "# MedSAM_CKPT_PATH = \"path/to/medsam_model_best.pth\"\n",
    "# sam_pretrain_path = \"path/to/medsam_vit_b.pth\"\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# medsam_model = sam_model_registry['vit_b'](checkpoint=sam_pretrain_path)\n",
    "\n",
    "# checkpoint = torch.load(MedSAM_CKPT_PATH, map_location=device)\n",
    "# start_epoch = checkpoint[\"epoch\"] + 1\n",
    "# medsam_model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# medsam_model = medsam_model.to(device)\n",
    "# medsam_model.eval()\n",
    "\n",
    "# print(f\"Model loaded successfully on {device}, trained until epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-header",
   "metadata": {},
   "source": [
    "## Example: Single Image Inference\n",
    "\n",
    "This demonstrates how to run inference on a single image with a bounding box prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess image\n",
    "image_path = 'path/to/your/image.png'\n",
    "img_cv = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Ensure image is 3-channel\n",
    "if len(img_cv.shape) == 2:\n",
    "    img_3c = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n",
    "else:\n",
    "    img_3c = img_cv\n",
    "\n",
    "# Convert from BGR to RGB\n",
    "img_3c = cv2.cvtColor(img_3c, cv2.COLOR_BGR2RGB)\n",
    "H, W, _ = img_3c.shape\n",
    "\n",
    "print(f\"Original image size: {H}x{W}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image to 1024x1024\n",
    "img_1024 = cv2.resize(img_3c, (1024, 1024), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Normalize the image to [0, 1] range\n",
    "img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "    img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    ")\n",
    "\n",
    "# Convert to tensor and prepare for model\n",
    "img_1024_tensor = torch.tensor(img_1024, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Preprocessed image tensor shape: {img_1024_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-bbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box in original image coordinates [x0, y0, x1, y1]\n",
    "# Example: box around center of image\n",
    "box_np = np.array([[W//4, H//4, 3*W//4, 3*H//4]])\n",
    "\n",
    "# Scale box to 1024x1024\n",
    "box_1024 = box_np / np.array([W, H, W, H]) * 1024\n",
    "\n",
    "print(f\"Bounding box (original scale): {box_np[0]}\")\n",
    "print(f\"Bounding box (1024 scale): {box_1024[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image embeddings\n",
    "with torch.no_grad():\n",
    "    image_embedding = medsam_model.image_encoder(img_1024_tensor)  # (1, 256, 64, 64)\n",
    "\n",
    "# Perform segmentation inference\n",
    "medsam_seg = medsam_inference(medsam_model, image_embedding, box_1024, 1024, 1024)\n",
    "\n",
    "print(f\"Segmentation mask shape: {medsam_seg.shape}\")\n",
    "print(f\"Number of positive pixels: {medsam_seg.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_3c)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Predicted mask\n",
    "axes[1].imshow(img_1024)\n",
    "show_mask(medsam_seg, axes[1])\n",
    "show_box(box_1024[0], axes[1])\n",
    "axes[1].set_title(\"Predicted Mask\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the segmentation mask\n",
    "output_path = 'output_mask.png'\n",
    "cv2.imwrite(output_path, medsam_seg * 255)  # Scale to 0-255 for saving\n",
    "print(f\"Mask saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
